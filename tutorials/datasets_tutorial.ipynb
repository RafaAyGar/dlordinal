{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing of Libraries\n",
    "\n",
    "From the *Ordinal Deep Learning* package, we import the methods that will allow us to work with ordinal datasets.\n",
    "\n",
    "We also import methods from libraries such as *pytorch* and *torchvision* that will allow us to process and work with the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlordinal.datasets import FGNet, Adience\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGNet\n",
    "\n",
    "To make use of the [FGNet dataset](https://yanweifu.github.io/FG_NET_data/), an instance of it will be created where the following fields will be specified:\n",
    "\n",
    "* __root__: an attribute that defines the path where the dataset will be downloaded and extracted.\n",
    "* __download__: an attribute that indicates the desire to perform the dataset download.\n",
    "* __process_data__: an attribute that allows indicating to the method whether the data should be preprocessed for working with it, in case the user does not want to perform their own preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n"
     ]
    }
   ],
   "source": [
    "fgnet = FGNet(root='./datasets/fgnet', download=True, process_data=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been downloaded, extracted, and preprocessed, we can load it to subsequently make use of it for training and validating a model.\n",
    "\n",
    "After decompressing the dataset and processing it, we will see that a folder named *FGNET* is created, and inside it, we will find the *train* and *test* folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageFolder(\n",
    "    root=\"./datasets/fgnet/FGNET/train\", transform=Compose([ToTensor()])\n",
    ")\n",
    "test_data = ImageFolder(\n",
    "    root=\"./datasets/fgnet/FGNET/test\", transform=Compose([ToTensor()])\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional data processing step, we are going to show how we can obtain the number of classes in the dataset and how we can create a partition for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the number of classes\n",
    "num_classes = len(train_data.classes)\n",
    "\n",
    "# Create a validation split\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=0)\n",
    "sss_splits = list(sss.split(X=np.zeros(len(train_data)), y=train_data.targets))\n",
    "train_idx, val_idx = sss_splits[0]\n",
    "\n",
    "# Create subsets for training and validation\n",
    "train_data = Subset(train_data, train_idx)\n",
    "val_data = Subset(train_data, val_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adience\n",
    "\n",
    "The [Adience dataset](https://talhassner.github.io/home/projects/Adience/Adience-data.html) does not allow direct download like FGNet, so a series of instructions must be followed to be able to download it.\n",
    "\n",
    "* Download files fold_0_data.txt-fold_4_data.txt and place in a common folder\n",
    "* Download aligned.tar.gz\n",
    "\n",
    "Once the instrucctions are followed, an instance of it will be created where the following fields will be specified:\n",
    "* __extract_file_path__: define the path where the file *aligned.tar.gz* is located.\n",
    "* __extract__: indicate to the methos if we want to extract the file *aligned.tar.gz*.\n",
    "* __folds_path__: indicate the path where text files with indices to the five-fold cross validation tests using all faces.\n",
    "* __images_path__: indicate the path where the extraction will be done.\n",
    "* __transformed_images_path__: indicate the path where all the images will be resized, maintaining the original aspect ratio, setting the height to 128 pixels, and allowing the width to adjust automatically.\n",
    "* __partition_path__: indicates the path where the images will be stored separated by age ranges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already extracted.\n",
      "Fold 0: discarding 104 entries (2.3%)\n",
      "Fold 1: discarding 456 entries (12.2%)\n",
      "Fold 2: discarding 594 entries (15.3%)\n",
      "Fold 3: discarding 377 entries (10.9%)\n",
      "Fold 4: discarding 137 entries (3.6%)\n",
      "Resizing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17702/17702 [06:47<00:00, 43.44it/s]\n",
      "20it [03:16,  9.84s/it]\n"
     ]
    }
   ],
   "source": [
    "adience = Adience(\n",
    "    extract_file_path=\"./datasets/adience/aligned.tar.gz\",\n",
    "    extract=True,\n",
    "    folds_path=\"./datasets/adience/folds\",\n",
    "    images_path=\"./datasets/adience/aligned\",\n",
    "    transformed_images_path=\"./datasets/adience/transformed_images\",\n",
    "    partition_path=\"./datasets/adience/partitions\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset has been extracted and the images have been processed and partitioned, the data is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImageFolder(\n",
    "    root=\"./datasets/adience/partitions\", transform=Compose([ToTensor()])\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, what has been loaded is the complete dataset, so a small code has been prepared to partition this data in a stratified way, making a *holout* in which 80% of the dataset images are for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "sss_splits = list(sss.split(X=np.zeros(len(data)), y=data.targets))\n",
    "train_idx, val_idx = sss_splits[0]\n",
    "\n",
    "# Create subsets for training and test\n",
    "train_data = Subset(train_data, train_idx)\n",
    "test_data = Subset(train_data, val_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "385611db6ca4af2663855b1744f455946eef985f7b33eb977c97667790417df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
