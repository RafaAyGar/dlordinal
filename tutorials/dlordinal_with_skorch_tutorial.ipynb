{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dlordinal.losses import TriangularCrossEntropyLoss\n",
    "from dlordinal.datasets import FGNet\n",
    "from torch import cuda, nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from skorch import NeuralNetClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess of FGNet dataset\n",
    "\n",
    "First, we present the configuration parameters for the experimentation and the number of workers for the `DataLoader`, which defines the number of subprocesses to use for data loading. In this specific case, it refers to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser_params = {\n",
    "    'lr': 1e-3,\n",
    "    'bs': 400,\n",
    "    'epochs': 5,\n",
    "    's': 2,\n",
    "    'c': 0.2,\n",
    "    'beta': 0.5\n",
    "}\n",
    "\n",
    "workers = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `FGNet` method to download and preprocess the images. Once that is done with the training data, we create a validation partition comprising 15% of the data using the `StratifiedShuffleSplit` method. Finally, with all the partitions, we load the images using a method called `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "fgnet = FGNet(root=\"./datasets/fgnet\", download=True, process_data=True)\n",
    "\n",
    "train_data = ImageFolder(\n",
    "    root=\"./datasets/fgnet/FGNET/train\", transform=Compose([ToTensor()])\n",
    ")\n",
    "test_data = ImageFolder(\n",
    "    root=\"./datasets/fgnet/FGNET/test\", transform=Compose([ToTensor()])\n",
    ")\n",
    "\n",
    "num_classes = len(train_data.classes)\n",
    "classes = train_data.classes\n",
    "targets = train_data.targets\n",
    "\n",
    "# Get CUDA device\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator\n",
    "\n",
    "We are setting up a deep learning model using `PyTorch` and `Skorch`. First, we define the model architecture using ResNet18, a pre-trained convolutional neural network, and customize its fully connected layer to match the number of classes in our classification task. Then we specify the loss function, in this case, a custom Triangular Cross Entropy Loss[1]. Finally, we configure the Skorch estimator, which serves as a bridge between PyTorch and scikit-learn, allowing us to train and evaluate our model seamlessly. We provide the model, loss function, and optimiser details such as the learning rate and number of epochs to the estimator. Additionally, we specify parameters for data loading and processing, like batch size and the number of workers, to optimise training performance.\n",
    "\n",
    "[1]: Víctor Manuel Vargas, Pedro Antonio Gutiérrez, Javier Barbero-Gómez, and César Hervás-Martínez (2023). *Soft Labelling Based on Triangular Distributions for Ordinal Classification.* Information Fusion, 93, 258--267. doi.org/10.1016/j.inffus.2023.01.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = TriangularCrossEntropyLoss(num_classes=num_classes).to(device)\n",
    "\n",
    "# Skorch estimator\n",
    "estimator = NeuralNetClassifier(\n",
    "    module=model,\n",
    "    criterion=loss_fn,\n",
    "    optimizer=Adam,\n",
    "    lr=optimiser_params[\"lr\"],\n",
    "    max_epochs=optimiser_params[\"epochs\"],\n",
    "    train_split=None,\n",
    "    callbacks=[],\n",
    "    device=device,\n",
    "    verbose=0,\n",
    "    iterator_train__batch_size=optimiser_params[\"bs\"],\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__num_workers=workers - 1,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_valid__batch_size=optimiser_params[\"bs\"],\n",
    "    iterator_valid__shuffle=False,\n",
    "    iterator_valid__num_workers=workers - 1,\n",
    "    iterator_valid__pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=6, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimator.fit(X=train_data, y=stargets)\n",
    "targets = np.array(targets)\n",
    "estimator.fit(X=train_data, y=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train probabilities = train_probs=array([[ 3.7356095 ,  2.280471  ,  0.27906656, -6.5274134 , -0.1423619 ,\n",
      "        -0.89688575],\n",
      "       [ 6.7312865 ,  4.2798862 , -2.754112  , -7.015324  , -0.58337   ,\n",
      "        -1.614481  ],\n",
      "       [ 1.5178611 ,  0.3035042 , -1.0939833 , -1.1187612 ,  0.3635073 ,\n",
      "        -1.4568175 ],\n",
      "       ...,\n",
      "       [-8.981531  , -2.1939955 , -1.2311378 , -1.599317  ,  1.7429321 ,\n",
      "         8.956122  ],\n",
      "       [-7.570979  , -2.4199474 , -0.9986418 ,  2.073321  ,  1.8904057 ,\n",
      "         3.514359  ],\n",
      "       [-4.612633  , -2.3110492 ,  1.4501587 ,  1.0073776 ,  0.30610457,\n",
      "         1.1957583 ]], dtype=float32)\n",
      "\n",
      "Test probabilities = test_probs=array([[-0.7816221 , -0.87308043, -1.196569  , -2.6637518 ,  1.0128176 ,\n",
      "         2.083984  ],\n",
      "       [-0.04164401,  1.2640952 ,  1.5022627 , -2.0729616 , -0.1945019 ,\n",
      "        -1.6816527 ],\n",
      "       [ 7.281721  ,  2.9113057 , -3.4834485 , -7.3575487 ,  1.2093832 ,\n",
      "        -1.4325407 ],\n",
      "       ...,\n",
      "       [-9.944385  , -3.6944542 , -0.42169666,  0.5072165 ,  2.8238878 ,\n",
      "         7.273284  ],\n",
      "       [-5.4416714 , -3.3233507 , -2.3007298 ,  0.98697877,  2.2850878 ,\n",
      "         4.3965707 ],\n",
      "       [-4.6799173 , -1.7463862 , -0.5284957 , -1.7213606 ,  0.89393014,\n",
      "         4.456833  ]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_probs = estimator.predict_proba(train_data)\n",
    "print(f\"Train probabilities = {train_probs=}\\n\")\n",
    "\n",
    "test_probs = estimator.predict_proba(test_data)\n",
    "print(f\"Test probabilities = {test_probs=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "385611db6ca4af2663855b1744f455946eef985f7b33eb977c97667790417df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
